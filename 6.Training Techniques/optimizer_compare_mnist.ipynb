{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========iteration:0===========\n",
      "SGD:2.4793477853087866\n",
      "Momentum:2.378656957870902\n",
      "AdaGrad:2.7306228981849765\n",
      "Adam:2.195724823003241\n",
      "===========iteration:100===========\n",
      "SGD:1.6657326187916768\n",
      "Momentum:0.3220152106327263\n",
      "AdaGrad:0.1641503628335228\n",
      "Adam:0.2437048340252706\n",
      "===========iteration:200===========\n",
      "SGD:0.8008886425154433\n",
      "Momentum:0.20891521542718755\n",
      "AdaGrad:0.07884540703857049\n",
      "Adam:0.13171135574491963\n",
      "===========iteration:300===========\n",
      "SGD:0.5253857854297265\n",
      "Momentum:0.17499406477607915\n",
      "AdaGrad:0.08162995104459606\n",
      "Adam:0.10413157056938158\n",
      "===========iteration:400===========\n",
      "SGD:0.42937224137471985\n",
      "Momentum:0.21044823032471094\n",
      "AdaGrad:0.1019022405859088\n",
      "Adam:0.2077314003582647\n",
      "===========iteration:500===========\n",
      "SGD:0.4338221354520376\n",
      "Momentum:0.17224632393513073\n",
      "AdaGrad:0.07687377219323319\n",
      "Adam:0.12511835409858327\n",
      "===========iteration:600===========\n",
      "SGD:0.42977725746077866\n",
      "Momentum:0.1728351503393628\n",
      "AdaGrad:0.07750002649274504\n",
      "Adam:0.12548465833647404\n",
      "===========iteration:700===========\n",
      "SGD:0.3091942867161237\n",
      "Momentum:0.14080953648120392\n",
      "AdaGrad:0.05166662977425281\n",
      "Adam:0.07594330312009537\n",
      "===========iteration:800===========\n",
      "SGD:0.21092047035801403\n",
      "Momentum:0.08947693099639042\n",
      "AdaGrad:0.03673286441568563\n",
      "Adam:0.07266056941460382\n",
      "===========iteration:900===========\n",
      "SGD:0.399041288372961\n",
      "Momentum:0.19234826847788006\n",
      "AdaGrad:0.06361012290662554\n",
      "Adam:0.11577542295039178\n",
      "===========iteration:1000===========\n",
      "SGD:0.2433037612201567\n",
      "Momentum:0.11214143290094514\n",
      "AdaGrad:0.06948428452943313\n",
      "Adam:0.06071969299628451\n",
      "===========iteration:1100===========\n",
      "SGD:0.2707760453999605\n",
      "Momentum:0.08795191026787055\n",
      "AdaGrad:0.04484555839573813\n",
      "Adam:0.04089491867897569\n",
      "===========iteration:1200===========\n",
      "SGD:0.3535422310407754\n",
      "Momentum:0.12799568374495257\n",
      "AdaGrad:0.05471321486093421\n",
      "Adam:0.1137764723254307\n",
      "===========iteration:1300===========\n",
      "SGD:0.20690534763097984\n",
      "Momentum:0.029867992416648014\n",
      "AdaGrad:0.024007245281150842\n",
      "Adam:0.02348985087506783\n",
      "===========iteration:1400===========\n",
      "SGD:0.3151417617610702\n",
      "Momentum:0.12532898073976276\n",
      "AdaGrad:0.031137473107458326\n",
      "Adam:0.12068400400485602\n",
      "===========iteration:1500===========\n",
      "SGD:0.32704956107684136\n",
      "Momentum:0.13523347532709684\n",
      "AdaGrad:0.04233157638499827\n",
      "Adam:0.07051056011908591\n",
      "===========iteration:1600===========\n",
      "SGD:0.20248567279089946\n",
      "Momentum:0.12287291931068586\n",
      "AdaGrad:0.05631888733240499\n",
      "Adam:0.09638095437274043\n",
      "===========iteration:1700===========\n",
      "SGD:0.15729516850399075\n",
      "Momentum:0.11710388398936933\n",
      "AdaGrad:0.057617476773373505\n",
      "Adam:0.06578260425536626\n",
      "===========iteration:1800===========\n",
      "SGD:0.11551290716874703\n",
      "Momentum:0.03640592610797654\n",
      "AdaGrad:0.014239642161712245\n",
      "Adam:0.020427761421810197\n",
      "===========iteration:1900===========\n",
      "SGD:0.16948325196134845\n",
      "Momentum:0.03798188394802275\n",
      "AdaGrad:0.020725008835741774\n",
      "Adam:0.017730547590731595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.util import smooth_curve\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import *\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 128\n",
    "max_iterations = 2000\n",
    "\n",
    "optimizers = {}\n",
    "optimizers['SGD'] = SGD()\n",
    "optimizers['Momentum'] = Momentum()\n",
    "optimizers['AdaGrad'] = AdaGrad()\n",
    "optimizers['Adam'] = Adam()\n",
    "#optimizers['RMSprop'] = RMSprop()\n",
    "\n",
    "networks = {}\n",
    "train_loss = {}\n",
    "for key in optimizers.keys():\n",
    "    networks[key] = MultiLayerNet(\n",
    "        input_size=784, hidden_size_list=[100, 100, 100, 100],\n",
    "        output_size=10)\n",
    "    train_loss[key] = []    \n",
    "\n",
    "\n",
    "for i in range(max_iterations):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    for key in optimizers.keys():\n",
    "        grads = networks[key].gradient(x_batch, t_batch)\n",
    "        optimizers[key].update(networks[key].params, grads)\n",
    "    \n",
    "        loss = networks[key].loss(x_batch, t_batch)\n",
    "        train_loss[key].append(loss)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print( \"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
    "        for key in optimizers.keys():\n",
    "            loss = networks[key].loss(x_batch, t_batch)\n",
    "            print(key + \":\" + str(loss))\n",
    "\n",
    "\n",
    "markers = {\"SGD\": \"o\", \"Momentum\": \"x\", \"AdaGrad\": \"s\", \"Adam\": \"D\"}\n",
    "x = np.arange(max_iterations)\n",
    "for key in optimizers.keys():\n",
    "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
